{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\L2RPN\\\\Dreamer_V3_Implimentation\\\\Dreamer-V3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.Reward import L2RPNSandBoxScore\n",
    "from lightsim2grid import LightSimBackend\n",
    "from dreamer.modules.worldModel import WorldModel\n",
    "from dreamer.Utils.utils import Config\n",
    "from dreamer.modules.actor_critic import ActorCritic\n",
    "from dreamer.modules.dreamerTrainer import DreamerTrainer\n",
    "from dreamer.Utils.utils import Config\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hrand\\anaconda3\\envs\\MARL2023paper_env\\Lib\\site-packages\\grid2op\\Backend\\backend.py:2062: UserWarning: The backend implementation you are using is probably too old to take advantage of the new feature added in grid2op 1.10.0: the possibility to have more than 2 busbars per substations (or not). To silence this warning, you can modify the `load_grid` implementation of your backend and either call:\n",
      "- self.can_handle_more_than_2_busbar if the current implementation    can handle more than 2 busbsars OR\n",
      "- self.cannot_handle_more_than_2_busbar if not.\n",
      "And of course, ideally, if the current implementation of your backend cannot handle more than 2 busbars per substation, then change it :-)\n",
      "Your backend will behave as if it did not support it.\n",
      "  warnings.warn(\"The backend implementation you are using is probably too old to take advantage of the \"\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = grid2op.make(\n",
    "    \"l2rpn_case14_sandbox\",\n",
    "    reward_class=L2RPNSandBoxScore,\n",
    "    backend=LightSimBackend()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is availabel for Training\n",
      "Model loaded successfully from dreamer\\models\\recurrent_model\n",
      "Model loaded successfully from dreamer\\models\\encoder\n",
      "Model loaded successfully from dreamer\\models\\dynamic_predictor\n",
      "Model loaded successfully from dreamer\\models\\reward_predictor\n",
      "Model loaded successfully from dreamer\\models\\continue_predictor\n",
      "Model loaded successfully from dreamer\\models\\decoder\n",
      "World model loaded at dreamer\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\recurrentModel.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\encoder.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\dynamics.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\rewardModel.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\continueModel.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "e:\\L2RPN\\Dreamer_V3_Implimentation\\Dreamer-V3\\dreamer\\modules\\decoder.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "c:\\Users\\hrand\\anaconda3\\envs\\MARL2023paper_env\\Lib\\site-packages\\grid2op\\Backend\\backend.py:2062: UserWarning: The backend implementation you are using is probably too old to take advantage of the new feature added in grid2op 1.10.0: the possibility to have more than 2 busbars per substations (or not). To silence this warning, you can modify the `load_grid` implementation of your backend and either call:\n",
      "- self.can_handle_more_than_2_busbar if the current implementation    can handle more than 2 busbsars OR\n",
      "- self.cannot_handle_more_than_2_busbar if not.\n",
      "And of course, ideally, if the current implementation of your backend cannot handle more than 2 busbars per substation, then change it :-)\n",
      "Your backend will behave as if it did not support it.\n",
      "  warnings.warn(\"The backend implementation you are using is probably too old to take advantage of the \"\n"
     ]
    }
   ],
   "source": [
    "config = Config.from_yaml(\"config.yml\")\n",
    "actor_critic = ActorCritic(config=config)\n",
    "world_model = WorldModel(config)\n",
    "world_model.load_world_model()\n",
    "trainer = DreamerTrainer(world_model=world_model, actor_critic=actor_critic,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "steps_per_epoch = 10\n",
    "import torch\n",
    "obs = env.reset()\n",
    "obs = obs.to_vect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.tensor(obs)\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "latent_states, hidden_states, actions, rewards, continues =trainer.imagine_trajectory(initial_state=obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of latent state:  16\n",
      "Size of hidden state:  16\n",
      "Size of actions:  16\n",
      "Size of rewards:  16\n",
      "Size of continues:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of latent state: \", len(latent_states))\n",
    "print(\"Size of hidden state: \", len(hidden_states))\n",
    "print(\"Size of actions: \", len(actions))\n",
    "print(\"Size of rewards: \", len(rewards))\n",
    "print(\"Size of continues: \", len(continues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  467.8332, -4184.3657, -4731.0391, -1196.9907,  3243.4456, -2850.3789,\n",
       "          3647.7180, -2546.7937,   844.2480,   928.1086,  2180.5015,  3202.4246,\n",
       "          1610.8192,  3428.7324,  2118.3696, -1491.5612, -3737.6987,   319.6416,\n",
       "          -675.1724, -3293.5249,  2917.8738,   145.6512, -2020.3873,  -124.2827,\n",
       "          1920.9397,   327.3677,  4241.2739,  -206.0128,  -678.0339, -1960.9033,\n",
       "          4373.1343,  3843.2397,  1646.9080,  1835.5894, -1349.0554,   295.2041,\n",
       "           766.0688,  2011.6403,  2642.6890, -3158.5127,  3055.8572,  1497.2297,\n",
       "         -2185.5283, -1372.6190,  3444.0374, -1188.3472,   231.3518, -1519.9739,\n",
       "          3495.4817, -1137.0533, -1348.1296, -4903.4585,  -994.5637, -3184.1550,\n",
       "         -2997.5564, -3103.1060,  2353.1155,  4944.1792,   510.8064, -2766.5117,\n",
       "         -3421.8718, -3327.9368,  3700.2791, -4429.6387]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_states[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 192])\n"
     ]
    }
   ],
   "source": [
    "# Stack all latent and hidden states from all timesteps\n",
    "latent_stack = torch.stack(latent_states)    # Shape: [time_steps, batch_size, latent_dim]\n",
    "hidden_stack = torch.stack(hidden_states)    # Shape: [time_steps, batch_size, hidden_dim]\n",
    "            \n",
    "# Concatenate latent and hidden states\n",
    "states = torch.cat([latent_stack, hidden_stack], dim=-1) \n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "trainer.initialize_buffer(num_trajectories=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 1, 192])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = trainer.replay_buffer.get_sample(batch_size=32)\n",
    "batch['states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = batch['states'][:, 0, :]  # [batch_size, obs_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 64])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of State:  torch.Size([1, 192])\n",
      "shape of Actiion:  torch.Size([1, 179])\n",
      "shape of Values torch.Size([16, 1, 1])\n",
      "shape of lambda Returns  torch.Size([16, 1, 1])\n",
      "States Shape in update critic torch.Size([16, 1, 192])\n",
      "Critic_loss_imagine:  5.541261672973633\n",
      "Replay Values :  tensor([[ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 3.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [-0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 0.0000],\n",
      "        [-2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 3.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 2.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 0.0000],\n",
      "        [-2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [-2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 3.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.5000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [-2.0000],\n",
      "        [ 1.5000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 2.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [-2.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.5000],\n",
      "        [ 0.0000],\n",
      "        [-1.0000],\n",
      "        [ 2.0000],\n",
      "        [ 1.5000],\n",
      "        [ 1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Replay Critic Loss :  1.6304636001586914\n",
      "Actor Loss :  183.8650665283203\n"
     ]
    }
   ],
   "source": [
    "ts = trainer.train_step(initial_state=obs, replay_buffer_batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL2023paper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
